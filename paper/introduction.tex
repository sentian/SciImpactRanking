 %!TEX root = ms.tex
\section*{Introduction}

Comparing the scientific impact of scholars or publications often occurs when making academic decisions. For instance, academic committees evaluate a candidate scholar relative to other cohorts in the same department to award tenure promotions. Directly comparing the number of citations can be biased, since the citations change with the seniority of the scholar. Another example is assigning research funding in which the scholarâ€™s portfolio is compared to other candidates from various facilities and disciplines. The magnitude of the citations that a publication receives varies drastically across disciplines. Evaluating whether to utilize citations favors scholars from more active fields and hence is not an appropriate measure.

To make the comparison feasible, we propose utilizing the rank percentile indicator, whose four fundamental elements are entity, benchmark, evaluation metric, and age. The entity can be either a publication (P) or a scholar (S). The benchmark (b) characterizes the reference set to which the entity is compared, and it is specified by the problem of interest. In the example of tenure promotions, the benchmark can comprise all the cohorts in the same department, while in the research funding allocation example, the benchmark contains all the candidates in competition. The cohorts in the benchmark are evaluated utilizing some metric (m), such as the number of citations, and the age $t$ specifies the time at which the evaluation is executed. The publication $j$ is ranked based on its evaluation metric at $t$ years since published, and the rank is further transformed into the rank percentile indicator, denoted as P$_{m}^{jb}(t)$. The rank percentile indicator for scholar $i$ can be calculated following the same procedure and is denoted as S$_{m}^{ib}(t)$.

The rank percentile indicator is significantly interpretable. It describes the performance of a publication or a scholar at certain age relative to the cohorts in the benchmark. Additionally, the rank percentile indicator is flexible in the choice of evaluation metric. For scholars, the h-index~\cite{hirsch2005index} is probably the most popular metric and is defined as the maximum number h for which the scholar has h publications, each with at least h citations. The h-index removes some of the bias introduced by utilizing the number of citations. A scholar participating in a small number of frequently cited works or a large number of low-profiled projects can have a high citation count but a low h-index, since h-index rewards a consistent stream of impactful efforts. The problem with the h-index is that the actual number of citations is irrelevant once it exceeds h, and hence, the h-index does not reward astonishing works differently than publications that attract barely sufficient citations to boost the h-index. Numerous indices have been proposed to improve the h-index, such as the g-index~\cite{egghe2006theory}, the m-index~\cite{hirsch2005index}, and the i-10 index~\cite{Connor2011}. All these metrics treat citations equally and do not distinguish between a citation from a highly regarded journal and a citation from a workshop panel. PageRank index~\cite{chen2007finding,walker2007ranking,ma2008bringing} utilizes the citation network and evaluates a publication by assigning different weights to its citations. It can further be aggregated to measure the impact of a scholar~\cite{senanayake2015pagerank}. However, there does not exist a single ideal metric, and all of the aforementioned metrics are biased in certain ways. 

The rank percentile normalizes the citations by their ranks relative to the citations of other publications in the benchmark. The usage of a normalized indicator to compare the performance of publications has been studied in the literature. A mean-based indicator normalizes the citations of publications in the benchmark with respect to the expected citation impact of the benchmark, which can be estimated by the arithmetic mean of citations for all publications in the benchmark~\cite{schubert1986relative}. Since the citation distribution is skewed and heavy-tailed, the arithmetic mean is not a reasonable representation of the expected citation impact, and therefore mean-based indicators can be largely influenced by a small number of frequently cited publications. These drawbacks can be largely avoided by utilizing the rank percentile indicator~\cite{bornmann2013use,mingers2015review,bornmann2019well}.

The projection of the future performance for a publication or a scholar is often of great interest for evaluation purposes. There have been extensive discussions regarding prediction of the number of citations and the resultant h-index score. The mechanism model unveils the factors driving the citation dynamic of publications, in which the three main factors are the scaling-law distribution of citations~\cite{price1976general,barabasi1999emergence,peterson2010nonuniversal,Radicchi2008}, aging~\cite{barabasi1999emergence,albert2002statistical,hajra2006modelling,dorogovtsev2000evolution}, and perceived novelty~\cite{Wang2013}. The mechanism model can be applied to predict the future evolution of citations~\cite{Wang2013}, but it relies on a long citation history~\cite{wang2014science,wang2014response}. Each publication must be addressed individually, and hence, it is not appropriate for large-scale analysis. Another type of predictive model formulates the task as a supervised learning problem. By utilizing sophisticated machine learning algorithms and an extensive list of features, these models can be utilized to predict citations~\cite{fu2008models,lokker2008prediction,ibanez2009predicting,mazloumian2012predicting,stern2014high,weihs2017learning} and h-index scores~\cite{hirsch2007does,acuna2012future,penner2013predictability,weihs2017learning}, and they can be scaled to account for large-scale datasets. 

To the best of our knowledge, little is known about the evolution of the rank percentile indicator over time and its predictive power. In this paper, we discuss the framework for calculating the rank percentile indicator. Additionally, we propose and justify a novel rank percentile indicator for scholars, and we demonstrate its advantage over rank percentiles based on the existing evaluation metrics. Furthermore, we study the predictability of the rank percentile indicator, illustrating that the publication percentile is highly stable over time, while the scholar percentile offers short-term stability and can be predicted via a simple linear regression model.


% complete references
\iffalse
scaling laws~\cite{price1976general,barabasi1999emergence,peterson2010nonuniversal,redner1998popular,redner2004citation,Radicchi2008,stringer2008effectiveness}
aging~\cite{barabasi1999emergence,albert2002statistical,boccaletti2006complex,krapivsky2001organization,newman2009first,hajra2004phase,hajra2005aging,hajra2006modelling,wang2008measuring,dorogovtsev2000evolution,dorogovtsev2001scaling,zhu2003effect}
\fi





