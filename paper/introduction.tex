 %!TEX root = main.tex
\section*{Introduction}

Quantifying the scientific impact is a long-standing challenge. The most frequently used metric is the number of citations\supercite{hirsch2005index,Radicchi2008,garfield1979citation,Garfield2006}. Despite its simplicity, citation counts have been criticized to be biased serving as the proxy for scientific importance, and various alternatives have been proposed in reliance on the citations. The most well-known metric is the h-index\supercite{hirsch2005index} that is defined as the maximum number $h$ for which the scholar has $h$ publications each with at least $h$ citations. A scholar only participating in a small number of highly cited works, or a large number of low-profiled papers, will have a high citation counts but a low h-index, since h-index rewards a consistent stream of impactful efforts. Since then, numerous indices have been proposed to remedy the drawbacks of h-index, e.g. g-index\supercite{egghe2006theory}, m-index\supercite{hirsch2005index} and i-10 index\supercite{Connor2011}. Unfortunately, these alternatives solve existing problems by creating new ones. For instance, the actual citations are irrelevant to the h-index once they exceeds h. On the contrary, g-index allows citations from highly cited works to boost the lower cited ones, but it can be saturated once the average number of citations per paper is larger than the number of papers. Furthermore, all these metrics treat citations equally, and do not distinguish between a citation from a highly regarded journal and a citation from a workshop panel. Pagerank-index\supercite{chen2007finding,walker2007ranking,ma2008bringing} utilizes the citation network and evaluates a publication by assigning different weights to its citations. It can further be aggregated to measure the impact of a scholar\supercite{senanayake2015pagerank}. However, there have been discussions about the dis-similarity between citation network and WWW where the PageRank algorithm is originally designed for\supercite{chen2007finding}.

% complete references
\iffalse
scaling laws\supercite{price1976general,barabasi1999emergence,peterson2010nonuniversal,redner1998popular,redner2004citation,Radicchi2008,stringer2008effectiveness}
aging\supercite{barabasi1999emergence,albert2002statistical,boccaletti2006complex,krapivsky2001organization,newman2009first,hajra2004phase,hajra2005aging,hajra2006modelling,wang2008measuring,dorogovtsev2000evolution,dorogovtsev2001scaling,zhu2003effect}
\fi

Predicting the scientific impact is the second challenge. To potentially assign fundings or tenures, the university committees not only evaluate a scholar's past achievements, but more importantly attempt to project the future success. Much attention have focused on understanding the mechanisms that drive the citation dynamics of publications. Three main factors include the citation distributions following scaling laws\supercite{price1976general,barabasi1999emergence,peterson2010nonuniversal,Radicchi2008}, aging\supercite{barabasi1999emergence,albert2002statistical,hajra2006modelling,dorogovtsev2000evolution}, and perceived novelty or importance\supercite{Wang2013}. The mechanistic model can be applied to predict the future citation evolutions of publications\supercite{Wang2013}. Unfortunately, the model estimation potentially relies on long citation history\supercite{wang2014science,wang2014response}, and each publication needs to be dealt with individually, hence it is not scalable for large-scale analysis. Another type of the predictive models formulates the task as a supervised learning problem. Indicators that can potentially drive the future impact are summarized as features in order to make predictions for metrics such as citations\supercite{fu2008models,lokker2008prediction,ibanez2009predicting,mazloumian2012predicting,stern2014high,weihs2017learning} and h-index\supercite{hirsch2007does,acuna2012future,penner2013predictability,weihs2017learning}. With the help of sophisticated machine learning algorithms, these models can be easily scaled to account for large-scale dataset.

Comparing the scientific impact is yet another challenge. How do we compare a publication from a less-liquid field say mathematics, with a publication in biology? Reference set or benchmark has been introduced to make such comparison feasible\supercite{vinkler2010evaluation}. The benchmark characterizes a specific field, a certain publishing year or an explicit document type. The citations of publications within each benchmark are normalized with respect to the benchmark. Mean-based indicators normalize the citations by the expected citation impact of the benchmark, that can be estimated by the arithmetic mean of citations for all publications in that benchmark\supercite{schubert1986relative}. As the citation distribution is skewed and heavy-tailed, the arithmetic mean is not a reasonable representation of the expected citation impact, and therefore mean-based indicators can be largely influenced by a small number of highly cited publications. Fortunately, these drawbacks can be largely avoided by using the rank percentile indicators\supercite{bornmann2013use,mingers2015review,bornmann2019well}, which normalize the citations by their rank relative to the citations of other publications in the benchmark. 

However, little is known about the evolution of the rank percentile indicator over time and its predictive power. In this paper, we connect the third challenge with the other two. We start from discussing a general framework of formulating a rank percentile indicator, that is flexible in terms of the evaluation metric for scientific impacts, the choice of benchmark, age and the entity of interest. We then study the pros and cons of different rank percentile indicators formulated using different evaluation metrics. Furthermore, we factor the age into rank percentile indicators and demonstrate that they have high predictive powers. The dataset that we use is a large-scale Google Scholar dataset containing scholars across all disciplines in the top US universities.




