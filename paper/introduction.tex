 %!TEX root = ms.tex
\section*{Introduction}

Comparisons of the scientific impact of scholars or publications often occur when making academic decisions. For instance, academic committees evaluate a candidate scholar relative to other cohorts in the same department to award tenure promotions. Directly comparing the number of citations can introduce bias, since the citations change with the seniority of the scholar. Another example is assigning research funding in which the scholarâ€™s portfolio is compared to other candidates from various facilities and disciplines. The magnitude of the citations that a publication receives varies drastically across disciplines; consequently, utilizing citations favors scholars from more active fields and hence is not an appropriate measure.

Rank percentile, a popular field- and time-normalized indicator, has been studied extensively in the application of comparing scientific impacts. When applied to the evaluation of a publication, it normalizes the citations that the publication has received by its rank relative to other publications in the benchmark, and the benchmark specifies a field or publication year. The rank percentile has a significant advantage over other types of normalized indicators: for instance, a mean-based indicator normalizes the citations of publications in the benchmark with respect to the expected citation impact of the benchmark, which can be estimated by the arithmetic mean of citations for all publications in the benchmark~\cite{schubert1986relative}. Since the citation distribution is skewed and heavy-tailed, the arithmetic mean is not a reasonable representation of the expected citation impact, and therefore, mean-based indicators can be largely influenced by a small number of frequently cited publications. These drawbacks are largely avoided by utilizing the rank percentile indicator~\cite{bornmann2013use,bornmann2015methods,mingers2015review,bornmann2019well,waltman2019field}, which has been claimed to be the most robust normalized indicator~\cite{hicks2015bibliometrics}. The rank percentile indicator can easily be adapted to identify top publications in a specific field or publication year~\cite{bornmann2014excellent}. It can be visualized utilizing a bar plot and beam plot along with statistical analysis, which provide a clear interpretation of the performance over time~\cite{bornmann2014distributions,bornmann2014evaluate,williams2014substantive,bornmann2018plots,bornmann2020evaluation}.

Predicting the evolution of an evaluation indicator is also of considerable interest for evaluation purposes. Extensive discussions have occurred regarding predictive models for bibliographic metrics. The mechanism model unveils the factors that drive the citation dynamic of publications; the main factors in this model are the scaling-law distribution of citations~\cite{price1976general,barabasi1999emergence,peterson2010nonuniversal,Radicchi2008}, aging~\cite{barabasi1999emergence,albert2002statistical,hajra2006modelling,dorogovtsev2000evolution}, and perceived novelty~\cite{Wang2013}. The mechanism model can be applied to predict the future evolution of citations~\cite{Wang2013}, but it relies on a long citation history~\cite{wang2014science,wang2014response}. Each publication must be addressed individually, and hence, it is not appropriate for large-scale analyses. Another type of predictive model formulates the task as a supervised learning problem. By employing sophisticated machine learning algorithms and extensive lists of features, these models can be utilized to predict citations~\cite{fu2008models,lokker2008prediction,ibanez2009predicting,mazloumian2012predicting,stern2014high,weihs2017learning} and h-index scores~\cite{hirsch2007does,acuna2012future,penner2013predictability,weihs2017learning,weis2021learning}, and they can be scaled to account for large-scale datasets. 

To the best of our knowledge, little is known about the evolution of the rank percentile indicator over time and its predictability. In this paper, we revisit the framework for calculating the rank percentile indicator. Additionally, we propose and justify a novel rank percentile indicator for scholars, and we demonstrate its advantage over traditional rank percentiles based on the existing bibliographic metrics. Furthermore, we study the predictability of the rank percentile indicators, illustrating that the publication percentile is highly stable over time, while the scholar percentile offers short-term stability and can be predicted via a simple linear regression model.

\iffalse
The rank percentile indicator is significantly interpretable. It describes the performance of a publication or a scholar at certain age relative to the cohorts in the benchmark. Additionally, h-index~\cite{hirsch2005index} is probably the most popular metric and is defined as the maximum number h for which the scholar has h publications, each with at least h citations. The h-index removes some of the bias introduced by utilizing the number of citations. A scholar participating in a small number of frequently cited works or a large number of low-profiled projects can have a high citation count but a low h-index, since h-index rewards a consistent stream of impactful efforts. The problem with the h-index is that the actual number of citations is irrelevant once it exceeds h, and hence, the h-index does not reward astonishing works differently than publications that attract barely sufficient citations to boost the h-index. Numerous indices have been proposed to improve the h-index, such as the g-index~\cite{egghe2006theory}, the m-index~\cite{hirsch2005index}, and the i-10 index~\cite{Connor2011}. All these metrics treat citations equally and do not distinguish between a citation from a highly regarded journal and a citation from a workshop panel. PageRank index~\cite{chen2007finding,walker2007ranking,ma2008bringing} utilizes the citation network and evaluates a publication by assigning different weights to its citations. It can further be aggregated to measure the impact of a scholar~\cite{senanayake2015pagerank}. However, there does not exist a single ideal metric, and all of the aforementioned metrics are biased in certain ways. 
\fi

% complete references
\iffalse
scaling laws~\cite{price1976general,barabasi1999emergence,peterson2010nonuniversal,redner1998popular,redner2004citation,Radicchi2008,stringer2008effectiveness}
aging~\cite{barabasi1999emergence,albert2002statistical,boccaletti2006complex,krapivsky2001organization,newman2009first,hajra2004phase,hajra2005aging,hajra2006modelling,wang2008measuring,dorogovtsev2000evolution,dorogovtsev2001scaling,zhu2003effect}
\fi





