 %!TEX root = ms.tex
\section*{Conclusion}

Rank percentile has been demonstrated to be a better indicator of the performance for a publication or a scholar compared to other types of field- and time- normalized indicators. Rank percentile is highly interpretable, and it provides flexibility in the choice of benchmark and evaluation metrics. In this paper, we proposed a novel rank percentile for scholars that has clear advantages over the traditional rank percentile, which is based on citation counts or h-index values. Furthermore, we focused on the time factor and studied the predictability of rank percentile. We illustrated that the rank percentile has significant predictive power. In particular, the publication percentile 
is highly stable over time, and the scholar percentile exhibits short-term stability. Although complex machine learning models that utilize an extensive list of features can provide slightly better predictive performance, the linear regression model with merely the autoregressive and difference features provides considerable prediction accuracy; thus indicating the ease of predicting rank percentile. In practice, the highly predictable rank percentile can be utilized in combination with other metrics to picture the trajectory of a scholar or a publication and assist in academic decision making. 


\section*{Limitation and Future Work}

A key limitation of our study is the survivorship bias built into the dataset of scholars. The dataset consisted of scholars who were either assistant, associate, or full professors in the top U.S. institutions through $2016$, and hence, we were more likely to include scholars who have been successful over the long term. We do not have a control set of researchers who have left the academic track by, for example, moving to the industry or failing to obtain tenureship. In future work, we should analyze which of the lower-performing junior scholars earn tenure and continue their academic career as well as which of them leave the academic track. We have some evidence that our metrics can be utilized for such predictions, which in turn can be utilized to control for the survivorship bias in our dataset.

Another limitation is the lack of thorough field-specific analyses. The rank percentile has been demonstrated to be problematic as a field-normalized indicator when papers cover multiple disciplines and when papers have multiple authors~\cite{bornmann2020evaluation}. A solution is to fractionally assign the papers to the disciplines or authors~\cite{waltman2011towards,waltman2015field}. Additionally, the rank percentile lacks cross-field and cross-scale stability~\cite{zitt2005relativity}. In our experiments, however, we did not notice a systematic difference between the field of biology and fields that encompass multiple disciplines. This does not mean that there are no field differences; it simply means that our dataset and analysis did not have the necessary power to clearly reveal the field-specific differences.

The bibliographic metrics considered in this paper that are the citation counts and h-index values, treat citations equally and do not distinguish between citations from highly regarded journals and citations from workshop panels. PageRank index~\cite{chen2007finding,walker2007ranking,ma2008bringing} utilizes the citation network and evaluates a publication by assigning different weights to its citations. In future work, it would be interesting to compare the PageRank-based percentile with the percentiles studied in this paper. 